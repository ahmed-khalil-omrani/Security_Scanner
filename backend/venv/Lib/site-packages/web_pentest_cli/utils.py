"""
Utility functions for web penetration testing CLI
"""

import re
import logging
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse, urljoin
from datetime import datetime

logger = logging.getLogger(__name__)


def is_valid_url(url: str) -> bool:
    """
    Validate if the given string is a valid URL.
    
    Args:
        url: URL string to validate
        
    Returns:
        True if valid URL, False otherwise
    """
    try:
        result = urlparse(url)
        return all([result.scheme in ('http', 'https'), result.netloc])
    except Exception:
        return False


def normalize_url(url: str) -> str:
    """
    Normalize URL by ensuring it has a scheme and removing fragments.
    
    Args:
        url: URL to normalize
        
    Returns:
        Normalized URL string
    """
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    parsed = urlparse(url)
    # Remove fragment
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"


def sanitize_for_logging(text: str, patterns: Optional[List[str]] = None) -> str:
    """
    Sanitize sensitive information from log output.
    
    Args:
        text: Text to sanitize
        patterns: Additional regex patterns to redact
        
    Returns:
        Sanitized text with sensitive data masked
    """
    if patterns is None:
        patterns = []
    
    # Default patterns for common sensitive data
    default_patterns = [
        (r'Authorization:\s+.*', 'Authorization: [REDACTED]'),  # âœ… Fixed - captures everything
        (r'Cookie:\s*[^\n]+', 'Cookie: [REDACTED]'),
        (r'Set-Cookie:\s*[^\n]+', 'Set-Cookie: [REDACTED]'),
        (r'api[_-]?key[\'"]?\s*[:=]\s*[\'"]?[\w-]+', 'api_key=[REDACTED]'),
        (r'token[\'"]?\s*[:=]\s*[\'"]?[\w.-]+', 'token=[REDACTED]'),
        (r'password[\'"]?\s*[:=]\s*[\'"]?[^\s&\'"]+', 'password=[REDACTED]'),
    ]
    
    sanitized = text
    for pattern, replacement in default_patterns:
        sanitized = re.sub(pattern, replacement, sanitized, flags=re.IGNORECASE)
    
    # Apply custom patterns
    for pattern in patterns:
        sanitized = re.sub(pattern, '[REDACTED]', sanitized)
    
    return sanitized


def extract_domain(url: str) -> str:
    """
    Extract domain from URL.
    
    Args:
        url: URL to extract domain from
        
    Returns:
        Domain name
    """
    parsed = urlparse(url)
    return parsed.netloc


def is_same_domain(url1: str, url2: str) -> bool:
    """
    Check if two URLs belong to the same domain.
    
    Args:
        url1: First URL
        url2: Second URL
        
    Returns:
        True if same domain, False otherwise
    """
    return extract_domain(url1) == extract_domain(url2)


def truncate_text(text: str, max_length: int = 500) -> str:
    """
    Truncate text to maximum length with ellipsis.
    
    Args:
        text: Text to truncate
        max_length: Maximum length
        
    Returns:
        Truncated text
    """
    if len(text) <= max_length:
        return text
    return text[:max_length] + "..."


def get_timestamp() -> str:
    """
    Get current timestamp in ISO format.
    
    Returns:
        ISO formatted timestamp string
    """
    return datetime.utcnow().isoformat() + 'Z'


def format_severity(severity: str) -> str:
    """
    Format severity level with color codes for console output.
    
    Args:
        severity: Severity level (info, low, medium, high, critical)
        
    Returns:
        Formatted severity string with ANSI color codes
    """
    colors = {
        'info': '\033[94m',      # Blue
        'low': '\033[92m',       # Green
        'medium': '\033[93m',    # Yellow
        'high': '\033[91m',      # Red
        'critical': '\033[95m',  # Magenta
    }
    reset = '\033[0m'
    
    color = colors.get(severity.lower(), '')
    return f"{color}{severity.upper()}{reset}"


def load_wordlist(filepath: str, max_items: int = 1000) -> List[str]:
    """
    Load wordlist from file with size limit.
    
    Args:
        filepath: Path to wordlist file
        max_items: Maximum number of items to load
        
    Returns:
        List of words
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            words = [line.strip() for line in f if line.strip()]
            if len(words) > max_items:
                logger.warning(f"Wordlist truncated to {max_items} items")
                return words[:max_items]
            return words
    except Exception as e:
        logger.error(f"Failed to load wordlist: {e}")
        return []


def merge_dicts(*dicts: Dict) -> Dict:
    """
    Merge multiple dictionaries.
    
    Args:
        *dicts: Variable number of dictionaries
        
    Returns:
        Merged dictionary
    """
    result = {}
    for d in dicts:
        result.update(d)
    return result


def parse_consent_file(filepath: str) -> Dict[str, Any]:
    """
    Parse consent file and validate required fields.
    
    Args:
        filepath: Path to consent file
        
    Returns:
        Dictionary with consent information
        
    Raises:
        ValueError: If consent file is invalid
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract required fields
        consent_data = {}
        
        # Look for target domain/URL
        target_match = re.search(r'TARGET[:\s]+(.+)', content, re.IGNORECASE)
        if target_match:
            consent_data['target'] = target_match.group(1).strip()
        
        # Look for signature/name
        sig_match = re.search(r'SIGNATURE[:\s]+(.+)', content, re.IGNORECASE)
        if sig_match:
            consent_data['signature'] = sig_match.group(1).strip()
        
        # Look for date
        date_match = re.search(r'DATE[:\s]+(.+)', content, re.IGNORECASE)
        if date_match:
            consent_data['date'] = date_match.group(1).strip()
        
        # Validate required fields
        required = ['target', 'signature', 'date']
        missing = [f for f in required if f not in consent_data]
        
        if missing:
            raise ValueError(f"Missing required consent fields: {', '.join(missing)}")
        
        consent_data['raw_content'] = content
        consent_data['validated_at'] = get_timestamp()
        
        return consent_data
        
    except FileNotFoundError:
        raise ValueError(f"Consent file not found: {filepath}")
    except Exception as e:
        raise ValueError(f"Failed to parse consent file: {e}")


def create_safe_filename(text: str) -> str:
    """
    Create safe filename from text by removing special characters.
    
    Args:
        text: Text to convert to filename
        
    Returns:
        Safe filename string
    """
    # Remove scheme and special characters
    safe = re.sub(r'[^a-zA-Z0-9._-]', '_', text)
    # Remove leading/trailing underscores
    safe = safe.strip('_')
    # Limit length
    return safe[:100]


class RateLimiter:
    """Simple rate limiter for request throttling."""
    
    def __init__(self, requests_per_second: float = 1.0):
        """
        Initialize rate limiter.
        
        Args:
            requests_per_second: Maximum requests per second
        """
        self.requests_per_second = requests_per_second
        self.min_interval = 1.0 / requests_per_second if requests_per_second > 0 else 0
        self.last_request_time = 0.0
    
    def wait_if_needed(self):
        """Wait if necessary to respect rate limit."""
        import time
        
        if self.min_interval <= 0:
            return
        
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_interval:
            sleep_time = self.min_interval - time_since_last
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
