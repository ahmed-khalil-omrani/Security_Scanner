"""
Link discovery, sitemap parsing, and reconnaissance module
"""

import logging
import re
from typing import List, Set, Dict, Optional
from urllib.parse import urljoin, urlparse, urlunparse
from bs4 import BeautifulSoup

from .http_client import HTTPClient
from .utils import is_same_domain, normalize_url

logger = logging.getLogger(__name__)


class Gatherer:
    """
    Performs passive reconnaissance and link discovery.
    """

    def __init__(self, http_client: HTTPClient, max_depth: int = 2):
        """
        Initialize gatherer.

        Args:
            http_client: HTTP client instance
            max_depth: Maximum crawl depth
        """
        self.client = http_client
        self.max_depth = max_depth
        self.discovered_urls = set()
        self.visited_urls = set()

    def gather_all(self, base_url: str) -> Dict[str, any]:
        """
        Perform all gathering operations.

        Args:
            base_url: Target base URL

        Returns:
            Dictionary with gathered information
        """
        logger.info(f"Starting reconnaissance on {base_url}")

        results = {
            'base_url': base_url,
            'robots_txt': self.fetch_robots_txt(base_url),
            'sitemap': self.fetch_sitemap(base_url),
            'discovered_urls': set(),
            'forms': [],
            'input_points': [],
            'technologies': set()
        }

        # Crawl the site
        urls = self.crawl(base_url, max_depth=self.max_depth)
        results['discovered_urls'] = urls

        # Extract forms and input points from discovered URLs
        for url in list(urls)[:50]:  # Limit to first 50 URLs
            forms, inputs = self.extract_forms_and_inputs(url)
            results['forms'].extend(forms)
            results['input_points'].extend(inputs)

        # Detect technologies
        response = self.client.get(base_url)
        if response:
            results['technologies'] = self.detect_technologies(response)

        logger.info(
            f"Reconnaissance complete: {len(results['discovered_urls'])} URLs, "
            f"{len(results['forms'])} forms, {len(results['input_points'])} input points"
        )

        return results

    def fetch_robots_txt(self, base_url: str) -> Optional[Dict]:
        """
        Fetch and parse robots.txt.

        Args:
            base_url: Target base URL

        Returns:
            Dictionary with robots.txt information or None
        """
        robots_url = urljoin(base_url, '/robots.txt')
        logger.debug(f"Fetching robots.txt from {robots_url}")

        response = self.client.get(robots_url)
        if not response or response.status_code != 200:
            return None

        content = response.text

        # Parse robots.txt
        disallowed = []
        allowed = []
        sitemaps = []

        for line in content.split('\n'):
            line = line.strip()
            if line.lower().startswith('disallow:'):
                path = line.split(':', 1)[1].strip()
                if path:
                    disallowed.append(path)
            elif line.lower().startswith('allow:'):
                path = line.split(':', 1)[1].strip()
                if path:
                    allowed.append(path)
            elif line.lower().startswith('sitemap:'):
                sitemap_url = line.split(':', 1)[1].strip()
                if sitemap_url:
                    sitemaps.append(sitemap_url)

        return {
            'url': robots_url,
            'disallowed_paths': disallowed,
            'allowed_paths': allowed,
            'sitemaps': sitemaps,
            'raw_content': content
        }

    def fetch_sitemap(self, base_url: str) -> Optional[Dict]:
        """
        Fetch and parse sitemap.xml.

        Args:
            base_url: Target base URL

        Returns:
            Dictionary with sitemap information or None
        """
        sitemap_url = urljoin(base_url, '/sitemap.xml')
        logger.debug(f"Fetching sitemap from {sitemap_url}")

        response = self.client.get(sitemap_url)
        if not response or response.status_code != 200:
            return None

        try:
            soup = BeautifulSoup(response.content, 'xml')
            urls = []

            for loc in soup.find_all('loc'):
                url = loc.text.strip()
                if url:
                    urls.append(url)

            return {
                'url': sitemap_url,
                'urls': urls,
                'count': len(urls)
            }
        except Exception as e:
            logger.warning(f"Failed to parse sitemap: {e}")
            return None

    def crawl(self, start_url: str, max_depth: int = 2) -> Set[str]:
        """
        Crawl website starting from URL.

        Args:
            start_url: Starting URL
            max_depth: Maximum crawl depth

        Returns:
            Set of discovered URLs
        """
        base_domain = urlparse(start_url).netloc

        to_visit = [(start_url, 0)]
        discovered = set()
        visited = set()

        while to_visit:
            current_url, depth = to_visit.pop(0)

            if depth > max_depth or current_url in visited:
                continue

            visited.add(current_url)
            discovered.add(current_url)

            # Fetch page
            response = self.client.get(current_url)
            if not response or response.status_code != 200:
                continue

            # Extract links
            links = self.extract_links(response.text, current_url)

            # Add same-domain links to queue
            for link in links:
                parsed_link = urlparse(link)
                if parsed_link.netloc == base_domain and link not in visited:
                    to_visit.append((link, depth + 1))

            # Limit total discovered URLs
            if len(discovered) >= 100:
                logger.info("Reached URL discovery limit (100)")
                break

        logger.debug(f"Crawl complete: discovered {len(discovered)} URLs")
        return discovered

    def extract_links(self, html: str, base_url: str) -> List[str]:
        """
        Extract links from HTML.

        Args:
            html: HTML content
            base_url: Base URL for resolving relative links

        Returns:
            List of absolute URLs
        """
        links = []

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Extract from <a> tags
            for tag in soup.find_all('a', href=True):
                href = tag['href']
                absolute_url = urljoin(base_url, href)
                # Remove fragments
                parsed = urlparse(absolute_url)
                clean_url = urlunparse(parsed._replace(fragment=''))
                links.append(clean_url)

            # Also check for links in <link>, <script>, <img> tags
            for tag in soup.find_all(['link', 'script', 'img'], src=True):
                src = tag.get('src') or tag.get('href')
                if src:
                    absolute_url = urljoin(base_url, src)
                    links.append(absolute_url)

        except Exception as e:
            logger.warning(f"Failed to extract links: {e}")

        return list(set(links))

    def extract_forms_and_inputs(self, url: str) -> tuple:
        """
        Extract forms and input parameters from URL.

        Args:
            url: Target URL

        Returns:
            Tuple of (forms, input_points)
        """
        forms = []
        input_points = []

        response = self.client.get(url)
        if not response or response.status_code != 200:
            return (forms, input_points)

        try:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract forms
            for form in soup.find_all('form'):
                form_data = {
                    'url': url,
                    'action': urljoin(url, form.get('action', '')),
                    'method': form.get('method', 'get').upper(),
                    'inputs': []
                }

                # Extract form inputs
                for input_tag in form.find_all(['input', 'textarea', 'select']):
                    input_info = {
                        'name': input_tag.get('name', ''),
                        'type': input_tag.get('type', 'text'),
                        'value': input_tag.get('value', '')
                    }
                    form_data['inputs'].append(input_info)

                forms.append(form_data)

            # Extract URL parameters as input points
            parsed = urlparse(url)
            if parsed.query:
                params = parsed.query.split('&')
                for param in params:
                    if '=' in param:
                        name = param.split('=')[0]
                        input_points.append({
                            'url': url,
                            'parameter': name,
                            'location': 'query'
                        })

        except Exception as e:
            logger.warning(f"Failed to extract forms: {e}")

        return (forms, input_points)

    def detect_technologies(self, response) -> Set[str]:
        """
        Detect web technologies from response headers and content.

        Args:
            response: HTTP response object

        Returns:
            Set of detected technologies
        """
        technologies = set()

        # Check headers
        headers = response.headers

        if 'X-Powered-By' in headers:
            technologies.add(f"X-Powered-By: {headers['X-Powered-By']}")

        if 'Server' in headers:
            technologies.add(f"Server: {headers['Server']}")

        # Check for common frameworks in HTML
        html = response.text

        if 'wp-content' in html or 'wp-includes' in html:
            technologies.add('WordPress')

        if 'drupal' in html.lower():
            technologies.add('Drupal')

        if 'joomla' in html.lower():
            technologies.add('Joomla')

        if 'react' in html.lower() or '_react' in html:
            technologies.add('React')

        if 'vue' in html.lower() or '__vue__' in html:
            technologies.add('Vue.js')

        if 'angular' in html.lower() or 'ng-' in html:
            technologies.add('Angular')

        return technologies
